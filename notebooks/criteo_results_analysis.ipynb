{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.30.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "import plotly.offline as pyo\n",
    "import plotly.express as px\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "from utils import analyze_results, plot_null_reports_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts on driving forces behind IPA, User-Epoch ARA, and Cookie Monster\n",
    "\n",
    "## System Differences\n",
    "1. IPA really suffers quickly. Each advertiser gets a budget across all their queries. It pays query budget in each epoch each time a query is run. So, all else fixed, a lot of queries on the same set of epochs will result in insufficient budget.\n",
    "2. Each advertiser query gets their own per-epoch budget in User-Epoch ARA. User-Epoch ARA pays query-epsilon amount for each epoch that is scans. So, it lasts longer than IPA.\n",
    "3. Each advertiser query gets their own per-epoch budget in Cookie Monster. However, unlike User-Epoch ARA, Cookie Monster only pays query-epsilon amount in an epoch if there is a relevant impression found within that epoch. So, it lasts longer than User-Epoch ARA.\n",
    "\n",
    "## Toggles to exploit differences\n",
    "1. Running lots of queries is going to show IPA run out of budget fast. We don't have to be clever about how we do this -- with the Criteo dataset, it happens a lot.\n",
    "2. Forcing User-Epoch ARA to scan a lot of epochs will force it to run out of budget faster than Cookie Monster. So, a mixture of reducing epoch size and increasing attribution window size should result in Cookie Monster performing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_bias_results(results: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    requested_workload_sizes = results.requested_workload_size.unique()\n",
    "    advertisers = results.destination.unique()\n",
    "    baselines = results.baseline.unique()\n",
    "    attribution_windows = results.num_days_attribution_window.unique()\n",
    "    epoch_sizes = results.num_days_per_epoch.unique()\n",
    "\n",
    "    records = []\n",
    "    for requested_workload_size in requested_workload_sizes:\n",
    "        for advertiser in advertisers:\n",
    "            for baseline in baselines:\n",
    "                for attribution_window in attribution_windows:\n",
    "                    for epoch_size in epoch_sizes:\n",
    "                        section = results[\n",
    "                            (results.baseline == baseline) &\n",
    "                            (results.destination == advertiser) &\n",
    "                            (results.requested_workload_size == requested_workload_size) &\n",
    "                            (results.num_days_attribution_window == attribution_window) &\n",
    "                            (results.num_days_per_epoch == epoch_size)\n",
    "                        ]\n",
    "                        for _, row in section.iterrows():\n",
    "                            accuracies = zip(row.e2e_bias_relative_accuracies, row.null_report_bias_relative_accuracies)\n",
    "                            for __, dimension in enumerate(accuracies):\n",
    "                                records.append({\n",
    "                                    \"e2e_bias_accuracy\": dimension[0],\n",
    "                                    \"null_report_bias_accuracy\": dimension[1],\n",
    "                                    \"requested_workload_size\": requested_workload_size,\n",
    "                                    \"advertiser\": advertiser,\n",
    "                                    \"baseline\": baseline,\n",
    "                                    \"attribution_window\": attribution_window,\n",
    "                                    \"epoch_size\": epoch_size,\n",
    "                                })\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Attribution Window Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sum for ('319A2412BDB0EF669733053640B80112',), __116\n",
      "0 sum for ('319A2412BDB0EF669733053640B80112',), __290\n",
      "0 sum for ('9D9E93D1D461D7BAE47FB67EC0E01B62',), __118\n",
      "0 sum for ('9D9E93D1D461D7BAE47FB67EC0E01B62',), __228\n",
      "0 sum for ('9FF550C0B17A3C493378CB6E2DEEE6E4',), __185\n",
      "0 sum for ('9FF550C0B17A3C493378CB6E2DEEE6E4',), __42\n",
      "0 sum for ('9FF550C0B17A3C493378CB6E2DEEE6E4',), __231\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __162\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __163\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __164\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __165\n",
      "0 sum for ('319A2412BDB0EF669733053640B80112',), __116\n",
      "0 sum for ('9D9E93D1D461D7BAE47FB67EC0E01B62',), __118\n",
      "0 sum for ('9FF550C0B17A3C493378CB6E2DEEE6E4',), __185\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __163\n",
      "0 sum for ('319A2412BDB0EF669733053640B80112',), __116\n",
      "0 sum for ('319A2412BDB0EF669733053640B80112',), __290\n",
      "0 sum for ('9D9E93D1D461D7BAE47FB67EC0E01B62',), __118\n",
      "0 sum for ('9D9E93D1D461D7BAE47FB67EC0E01B62',), __228\n",
      "0 sum for ('9FF550C0B17A3C493378CB6E2DEEE6E4',), __185\n",
      "0 sum for ('9FF550C0B17A3C493378CB6E2DEEE6E4',), __42\n",
      "0 sum for ('9FF550C0B17A3C493378CB6E2DEEE6E4',), __231\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __162\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __163\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __164\n",
      "0 sum for ('E3DDEB04F8AFF944B11943BB57D2F620',), __165\n"
     ]
    }
   ],
   "source": [
    "path = \"ray/criteo/bias_varying_attribution_window_size\"\n",
    "t = .90\n",
    "results = analyze_results(path, \"bias\", parallelize=False, t=t)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = from_bias_results(results)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "Plot the CDFs for each attribution window size and save them to pngs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Epoch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ray/criteo/bias_varying_epoch_size\"\n",
    "t = .90\n",
    "results = analyze_results(path, \"bias\", parallelize=False, t=t)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = from_bias_results(results)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "Plot the CDFs for each epoch size and save them to pngs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Epoch and Attribution Window Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ray/criteo/bias_varying_epoch_and_attribution_window_size\"\n",
    "t = .90\n",
    "results = analyze_results(path, \"bias\", parallelize=False, t=t)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = from_bias_results(results)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "Plot the CDFs for each epoch and attribution window size combination and save them to pngs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Number of Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ray/criteo/bias_varying_num_queries\"\n",
    "t = .90\n",
    "results = analyze_results(path, \"bias\", parallelize=False, t=t)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "1. Plot fraction of null reports over workload sizes\n",
    "2. Plot average relative accuracy of null reports over workload sizes\n",
    "3. Plot RMSE of relative accuracy over workload sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workload_size = 320\n",
    "# attribution_window = 90\n",
    "# ds = df.loc[(df.requested_workload_size == workload_size) & (df.e2e_bias_accuracy >= 0) & (df.attribution_window == attribution_window)]\n",
    "# ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for log_y in [True, False]:\n",
    "#     title = (\n",
    "#         f\"Zoomed in CDF for relative accuracy (workload size {workload_size} attribution window {attribution_window})\"\n",
    "#         if log_y else\n",
    "#         f\"CDF for relative accuracy (workload size {workload_size} attribution window {attribution_window})\"\n",
    "#     )\n",
    "#     filename = (\n",
    "#         f\"cdf_zoomed_relative_accuracy_ws_{workload_size}_aw_{attribution_window}\"\n",
    "#         if log_y else\n",
    "#         f\"cdf_relative_accuracy_ws_{workload_size}_aw_{attribution_window}\"\n",
    "#     )\n",
    "#     figcdf = px.ecdf(\n",
    "#         ds,\n",
    "#         y=\"e2e_bias_accuracy\",\n",
    "#         orientation='h',\n",
    "#         color=\"baseline\",\n",
    "#         log_y=log_y,\n",
    "        \n",
    "#     )\n",
    "#     figcdf.update_layout(\n",
    "#         title=title,\n",
    "#         xaxis_title=\"proportion of queries\",\n",
    "#         yaxis_title=\"relative accuracy\"\n",
    "#     )\n",
    "#     figcdf.show()\n",
    "#     figcdf.write_image(f\"./large/{filename}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for log_y in [True, False]:\n",
    "#     title = (\n",
    "#         f\"Zoomed in CDF for null report bias accuracy (workload size {workload_size} attribution window {attribution_window})\"\n",
    "#         if log_y else\n",
    "#         f\"CDF for null report accuracy (workload size {workload_size} attribution window {attribution_window})\"\n",
    "#     )\n",
    "#     filename = (\n",
    "#         f\"cdf_zoomed_null_report_bias_relative_accuracy_ws_{workload_size}_as_{attribution_window}\"\n",
    "#         if log_y else\n",
    "#         f\"cdf_null_report_bias_relative_accuracy_ws_{workload_size}_as_{attribution_window}\"\n",
    "#     )\n",
    "#     figcdf = px.ecdf(\n",
    "#         ds,\n",
    "#         y=\"null_report_bias_accuracy\",\n",
    "#         orientation='h',\n",
    "#         color=\"baseline\",\n",
    "#         log_y=log_y,\n",
    "        \n",
    "#     )\n",
    "#     figcdf.update_layout(\n",
    "#         title=title,\n",
    "#         xaxis_title=\"proportion of queries\",\n",
    "#         yaxis_title=\"relative accuracy\"\n",
    "#     )\n",
    "#     figcdf.show()\n",
    "#     figcdf.write_image(f\"./large/{filename}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[df.e2e_bias_accuracy < 0].shape)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for requested_workload_size in m.requested_workload_size.unique():\n",
    "#     chunk = m.loc[(m.requested_workload_size == requested_workload_size)]\n",
    "#     fig = px.line(\n",
    "#         chunk,\n",
    "#         x=\"proportion_of_queries\",\n",
    "#         y=\"relative_accuracy\",\n",
    "#         range_y=[0, 1],\n",
    "#         range_x=[0, 1],\n",
    "#         color=\"baseline\",\n",
    "#         markers=True,\n",
    "#         title=f\"average relative accuracy per proportion of queries by baseline for workload size {requested_workload_size}\"\n",
    "\n",
    "#     )\n",
    "#     pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_df = results.sort_values(by=[\"key\", \"baseline\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workload_sizes = [5, 10, 15, 20, 25, 30]\n",
    "# for requested_workload_size in workload_sizes:\n",
    "#     content = sorted_df.loc[sorted_df.requested_workload_size == requested_workload_size]\n",
    "#     fig1 = px.bar(\n",
    "#         content,\n",
    "#         x=\"destination\",\n",
    "#         y=\"fraction_queries_relatively_accurate_e2e\",\n",
    "#         color=\"baseline\",\n",
    "#         barmode=\"group\",\n",
    "#         title=f\"Frac. queries with >= {t * 100}% rel. accuracy by destination (workload size {requested_workload_size})\"\n",
    "#     )\n",
    "#     fig2 = px.bar(\n",
    "#         content,\n",
    "#         x=\"destination\",\n",
    "#         y=\"e2e_bias_average_relative_accuracy\",\n",
    "#         color=\"baseline\",\n",
    "#         barmode=\"group\",\n",
    "#         title=f\"Avg. rel. accuracy accross queries by destination (workload size {requested_workload_size})\"\n",
    "#     )\n",
    "#     pyo.iplot(fig1)\n",
    "#     pyo.iplot(fig2)\n",
    "#     fig1.write_image(f\"./large/e2e_bias_fraction_relative_accuracy_ws_{requested_workload_size}.png\")\n",
    "#     fig2.write_image(f\"./large/e2e_bias_average_relative_accuracy_ws_{requested_workload_size}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for destination in results.groupby(['destination']).destination.unique():\n",
    "#     advertiser = results[results['destination'].isin(destination)]\n",
    "#     plot_null_reports_analysis(advertiser, save_dir=\"large\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
